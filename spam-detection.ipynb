{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d00d20e",
   "metadata": {},
   "source": [
    "# **Spam Email Detection â€” Naive Bayes (Machine Learning)**\n",
    "\n",
    "This notebook contains a simple, reproducible implementation of an email spam detection pipeline using the Naive Bayes classifier. The project uses the \"Spam Email Classification\" dataset from Kaggle and demonstrates data loading, preprocessing, model training, evaluation, and basic model export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1fba",
   "metadata": {},
   "source": [
    "## **Step 00** : Install nessessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4541d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types==0.7.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.11.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: click==8.3.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (8.3.0)\n",
      "Requirement already satisfied: comm==0.2.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: debugpy==1.8.17 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.8.17)\n",
      "Requirement already satisfied: decorator==5.2.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup==1.3.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: executing==2.2.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: fastapi==0.119.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.119.1)\n",
      "Requirement already satisfied: idna==3.11 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.11)\n",
      "Requirement already satisfied: ipykernel==7.0.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (7.0.1)\n",
      "Requirement already satisfied: ipython==8.37.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (8.37.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.19.2)\n",
      "Requirement already satisfied: joblib==1.5.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.9.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.6.0)\n",
      "Requirement already satisfied: nltk==3.9.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (3.9.2)\n",
      "Requirement already satisfied: numpy==2.2.6 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (2.2.6)\n",
      "Requirement already satisfied: packaging==25.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2.3.3)\n",
      "Requirement already satisfied: parso==0.8.5 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.8.5)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.5.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (4.5.0)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (3.0.52)\n",
      "Requirement already satisfied: psutil==7.1.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (7.1.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.12.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (2.12.3)\n",
      "Requirement already satisfied: pydantic_core==2.41.4 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (2.41.4)\n",
      "Requirement already satisfied: Pygments==2.19.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (2025.2)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (27.1.0)\n",
      "Requirement already satisfied: regex==2025.9.18 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (2025.9.18)\n",
      "Requirement already satisfied: six==1.17.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (1.3.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (0.6.3)\n",
      "Requirement already satisfied: starlette==0.48.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (0.48.0)\n",
      "Requirement already satisfied: tornado==6.5.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (6.5.2)\n",
      "Requirement already satisfied: uvicorn==0.27.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (0.27.0)\n",
      "Requirement already satisfied: python-multipart==0.0.5 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.0.5)\n",
      "Requirement already satisfied: requests==2.31.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (2.31.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspection==0.4.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (0.4.2)\n",
      "Requirement already satisfied: typing_extensions==4.15.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (4.15.0)\n",
      "Requirement already satisfied: tzdata==2025.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (2025.2)\n",
      "Requirement already satisfied: wcwidth==0.2.14 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (0.2.14)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.6.0)\n",
      "Requirement already satisfied: nltk==3.9.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (3.9.2)\n",
      "Requirement already satisfied: numpy==2.2.6 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (2.2.6)\n",
      "Requirement already satisfied: packaging==25.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2.3.3)\n",
      "Requirement already satisfied: parso==0.8.5 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.8.5)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.5.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (4.5.0)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (3.0.52)\n",
      "Requirement already satisfied: psutil==7.1.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (7.1.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.12.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (2.12.3)\n",
      "Requirement already satisfied: pydantic_core==2.41.4 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (2.41.4)\n",
      "Requirement already satisfied: Pygments==2.19.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (2025.2)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (27.1.0)\n",
      "Requirement already satisfied: regex==2025.9.18 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (2025.9.18)\n",
      "Requirement already satisfied: six==1.17.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (1.3.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (0.6.3)\n",
      "Requirement already satisfied: starlette==0.48.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (0.48.0)\n",
      "Requirement already satisfied: tornado==6.5.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (6.5.2)\n",
      "Requirement already satisfied: uvicorn==0.27.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (0.27.0)\n",
      "Requirement already satisfied: python-multipart==0.0.5 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.0.5)\n",
      "Requirement already satisfied: requests==2.31.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (2.31.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspection==0.4.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (0.4.2)\n",
      "Requirement already satisfied: typing_extensions==4.15.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (4.15.0)\n",
      "Requirement already satisfied: tzdata==2025.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (2025.2)\n",
      "Requirement already satisfied: wcwidth==0.2.14 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (0.2.14)\n",
      "Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.10/site-packages (from uvicorn==0.27.0->-r requirements.txt (line 43)) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (2.5.0)\n",
      "Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.10/site-packages (from uvicorn==0.27.0->-r requirements.txt (line 43)) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 45)) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72b997",
   "metadata": {},
   "source": [
    "## **Step 01** : Data loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a50ef320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ezzoubair/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load Data from CSV file\n",
    "df = pd.read_csv(\"data/email.csv\")\n",
    "\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "train_size = int(0.8 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf8917b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34750/3905594788.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"category\"]=train_data[\"category\"].map({'spam': 1, 'ham': 0})\n",
      "/tmp/ipykernel_34750/3905594788.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[\"category\"]=test_data[\"category\"].map({'spam': 1, 'ham': 0})\n"
     ]
    }
   ],
   "source": [
    " # Change the category to a binary values (0 or 1) based on the message is spam (1) or not spam (0) \n",
    "train_data[\"category\"]=train_data[\"category\"].map({'spam': 1, 'ham': 0})\n",
    "test_data[\"category\"]=test_data[\"category\"].map({'spam': 1, 'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ce14578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34750/128791528.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"message\"] = train_data[\"message\"].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and processing Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation using regEx\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    # Remove stopwords and short words (optional: words <= 2 chars)\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "    return words\n",
    "\n",
    "train_data[\"message\"] = train_data[\"message\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39255efd",
   "metadata": {},
   "source": [
    "**And that it for the data manipulation we need for now !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acfd0a",
   "metadata": {},
   "source": [
    "## **Step 02** : Feature Extraction for Text\n",
    "in other words, we need to extract meaning from the data we made !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d01ab6",
   "metadata": {},
   "source": [
    "### Create vocabolary from messages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0f508b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "7270\n",
      "7270\n"
     ]
    }
   ],
   "source": [
    "# merge all tokens into one  big list\n",
    "messages_tokens = sum(train_data[\"message\"],[])\n",
    "print(len(messages_tokens))\n",
    "\n",
    "# Eleminate duplicate\n",
    "vocabulary = list(set(messages_tokens))\n",
    "\n",
    "vocab_array = np.array(vocabulary)\n",
    "\n",
    "print(len(vocab_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6db2480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34750/2488823393.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"vector\"] = train_data[\"message\"].apply(vectorize_message)\n",
      "/tmp/ipykernel_34750/2488823393.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[\"vector\"] = test_data[\"message\"].apply(vectorize_message)\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {word: i for i, word in enumerate(vocab_array)}\n",
    "\n",
    "def vectorize_message(message):\n",
    "    vec = np.zeros(len(vocab_array), dtype=int)\n",
    "    for w in message:\n",
    "        if w in word_to_idx:\n",
    "            vec[word_to_idx[w]] += 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "train_data[\"vector\"] = train_data[\"message\"].apply(vectorize_message)\n",
    "test_data[\"vector\"] = test_data[\"message\"].apply(vectorize_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e570791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category = np.array(train_data[\"category\"])\n",
    "train_matrix = np.array(train_data[\"vector\"])\n",
    "\n",
    "def train_NB_0(train_matrix,train_category):\n",
    "    numTrainDocs = len(train_matrix)\n",
    "    numWords = len(train_matrix[0])\n",
    "    pSpam = sum(train_category) / float(numTrainDocs)\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        if train_category[i] == 1:\n",
    "            p1Num += train_matrix[i]\n",
    "            p1Denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p0Num += train_matrix[i]\n",
    "            p0Denom += sum(train_matrix[i])\n",
    "\n",
    "    p1Vect = (1 + p1Num) / (2 + p1Denom)\n",
    "    p0Vect = (1 + p0Num) / (2 + p0Denom)\n",
    "    return p0Vect, p1Vect, pSpam\n",
    "\n",
    "\n",
    "def classify_NB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * np.log(p1Vec)) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * np.log(p0Vec)) + np.log(1.0 - pClass1)\n",
    "    return 1 if p1 > p0 else 0\n",
    "\n",
    "\n",
    "p0V, p1V, pSpam = train_NB_0(train_matrix, train_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1012b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test labels and vectors\n",
    "test_labels = np.array(test_data[\"category\"])\n",
    "test_vectors = np.array(test_data[\"vector\"])\n",
    "\n",
    "# Classify each test message\n",
    "predictions = np.array([classify_NB(vec, p0V, p1V, pSpam) for vec in test_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e4c31562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "tp = np.sum((predictions == 1) & (test_labels == 1))\n",
    "fp = np.sum((predictions == 1) & (test_labels == 0))\n",
    "tn = np.sum((predictions == 0) & (test_labels == 0))\n",
    "fn = np.sum((predictions == 0) & (test_labels == 1))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (tp + tn) / len(test_labels)\n",
    "\n",
    "# Precision\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "# Recall\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# F1-Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4bd51",
   "metadata": {},
   "source": [
    "## **Step 03**: Model Evaluation and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a455dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction distribution:\n",
      "Predicted Ham (0): 1115\n",
      "Predicted Spam (1): 0\n",
      "Total predictions: 1115\n",
      "\n",
      "Actual distribution in test set:\n",
      "Actual Ham (0): 970\n",
      "Actual Spam (1): 145\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives (Spam correctly identified): 0\n",
      "False Positives (Ham incorrectly identified as Spam): 0\n",
      "True Negatives (Ham correctly identified): 970\n",
      "False Negatives (Spam incorrectly identified as Ham): 145\n",
      "\n",
      "Test data shape: (1115, 3)\n",
      "Test vectors shape: (1115,)\n",
      "Vocabulary size: 7270\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check what the model is predicting\n",
    "print(\"Prediction distribution:\")\n",
    "print(f\"Predicted Ham (0): {np.sum(predictions == 0)}\")\n",
    "print(f\"Predicted Spam (1): {np.sum(predictions == 1)}\")\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "\n",
    "print(\"\\nActual distribution in test set:\")\n",
    "print(f\"Actual Ham (0): {np.sum(test_labels == 0)}\")\n",
    "print(f\"Actual Spam (1): {np.sum(test_labels == 1)}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"True Positives (Spam correctly identified): {tp}\")\n",
    "print(f\"False Positives (Ham incorrectly identified as Spam): {fp}\")\n",
    "print(f\"True Negatives (Ham correctly identified): {tn}\")\n",
    "print(f\"False Negatives (Spam incorrectly identified as Ham): {fn}\")\n",
    "\n",
    "# Check if the test data has the correct preprocessing\n",
    "print(f\"\\nTest data shape: {test_data.shape}\")\n",
    "print(f\"Test vectors shape: {test_vectors.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocab_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae51a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample test message before cleaning:\n",
      "category                                                    0\n",
      "message     If you want to mapquest it or something look u...\n",
      "vector      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: 4457, dtype: object\n",
      "\n",
      "Cleaning test data messages...\n",
      "\n",
      "Sample test message after cleaning:\n",
      "category                                                    0\n",
      "message     [want, mapquest, something, look, dogwood, dri...\n",
      "vector      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: 4457, dtype: object\n",
      "\n",
      "Re-vectorizing test data...\n",
      "Updated test vectors shape: (1115,)\n"
     ]
    }
   ],
   "source": [
    "# Check if test data preprocessing is missing\n",
    "print(\"Sample test message before cleaning:\")\n",
    "print(test_data.iloc[0])\n",
    "\n",
    "# Fix: Clean test data messages (this was missing!)\n",
    "print(\"\\nCleaning test data messages...\")\n",
    "test_data = test_data.copy()  # Avoid SettingWithCopyWarning\n",
    "test_data[\"message\"] = test_data[\"message\"].apply(clean_text)\n",
    "\n",
    "print(\"\\nSample test message after cleaning:\")\n",
    "print(test_data.iloc[0])\n",
    "\n",
    "# Re-vectorize test data with cleaned messages\n",
    "print(\"\\nRe-vectorizing test data...\")\n",
    "test_data[\"vector\"] = test_data[\"message\"].apply(vectorize_message)\n",
    "\n",
    "# Update test vectors\n",
    "test_vectors = np.array(test_data[\"vector\"])\n",
    "print(f\"Updated test vectors shape: {test_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fa91b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CORRECTED RESULTS:\n",
      "==================================================\n",
      "Prediction distribution:\n",
      "Predicted Ham (0): 952\n",
      "Predicted Spam (1): 163\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 136\n",
      "False Positives: 27\n",
      "True Negatives: 943\n",
      "False Negatives: 9\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 0.968\n",
      "Precision: 0.834\n",
      "Recall: 0.938\n",
      "F1-Score: 0.883\n"
     ]
    }
   ],
   "source": [
    "# Re-run predictions with properly cleaned test data\n",
    "predictions = np.array([classify_NB(vec, p0V, p1V, pSpam) for vec in test_vectors])\n",
    "\n",
    "# Recalculate metrics\n",
    "tp = np.sum((predictions == 1) & (test_labels == 1))\n",
    "fp = np.sum((predictions == 1) & (test_labels == 0))\n",
    "tn = np.sum((predictions == 0) & (test_labels == 0))\n",
    "fn = np.sum((predictions == 0) & (test_labels == 1))\n",
    "\n",
    "# Updated metrics\n",
    "accuracy = (tp + tn) / len(test_labels)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CORRECTED RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Prediction distribution:\")\n",
    "print(f\"Predicted Ham (0): {np.sum(predictions == 0)}\")\n",
    "print(f\"Predicted Spam (1): {np.sum(predictions == 1)}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a2be8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Message Testing:\n",
      "============================================================\n",
      "Message: FREE! Claim your prize now! Limited time offer!\n",
      "Cleaned words: ['free', 'claim', 'prize', 'limited', 'time', 'offer']\n",
      "Vector sum: 6\n",
      "Prediction: SPAM\n",
      "------------------------------------------------------------\n",
      "Message: Hey, want to grab lunch tomorrow?\n",
      "Cleaned words: ['want', 'grab', 'lunch', 'tomorrow']\n",
      "Vector sum: 4\n",
      "Prediction: HAM\n",
      "------------------------------------------------------------\n",
      "Message: WINNER! You've won $1000! Call now!\n",
      "Cleaned words: ['winner', 'youve', '1000', 'call']\n",
      "Vector sum: 4\n",
      "Prediction: SPAM\n",
      "------------------------------------------------------------\n",
      "Message: Meeting at 3pm in conference room B\n",
      "Cleaned words: ['meeting', 'conference', 'room']\n",
      "Vector sum: 3\n",
      "Prediction: HAM\n",
      "------------------------------------------------------------\n",
      "Message: Congratulations! Click here to claim your reward\n",
      "Cleaned words: ['congratulations', 'click', 'claim', 'reward']\n",
      "Vector sum: 4\n",
      "Prediction: SPAM\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test individual messages to verify the model works\n",
    "test_messages = [\n",
    "    \"FREE! Claim your prize now! Limited time offer!\",\n",
    "    \"Hey, want to grab lunch tomorrow?\",\n",
    "    \"WINNER! You've won $1000! Call now!\",\n",
    "    \"Meeting at 3pm in conference room B\",\n",
    "    \"Congratulations! Click here to claim your reward\"\n",
    "]\n",
    "\n",
    "print(\"Individual Message Testing:\")\n",
    "print(\"=\"*60)\n",
    "for msg in test_messages:\n",
    "    cleaned = clean_text(msg)\n",
    "    vectorized = vectorize_message(cleaned)\n",
    "    prediction = classify_NB(vectorized, p0V, p1V, pSpam)\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Cleaned words: {cleaned}\")\n",
    "    print(f\"Vector sum: {vectorized.sum()}\")\n",
    "    print(f\"Prediction: {'SPAM' if prediction == 1 else 'HAM'}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762545f",
   "metadata": {},
   "source": [
    "## **Step 04**: Model Persistence\n",
    "\n",
    "Save the trained model and components for later use in production or API deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "41c6ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n",
      "Saved components:\n",
      "- Ham probability vector: 7270 features\n",
      "- Spam probability vector: 7270 features\n",
      "- Prior spam probability: 0.135\n",
      "- Vocabulary size: 7270\n",
      "- Model file: models/spam_classifier_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the trained model components\n",
    "model_data = {\n",
    "    'p0_vector': p0V,  # Ham probabilities\n",
    "    'p1_vector': p1V,  # Spam probabilities\n",
    "    'p_spam': pSpam,   # Prior probability of spam\n",
    "    'vocabulary': vocab_array,\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'stop_words': stop_words\n",
    "}\n",
    "\n",
    "# Save model to pickle file\n",
    "with open('models/spam_classifier_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(\"Saved components:\")\n",
    "print(f\"- Ham probability vector: {len(p0V)} features\")\n",
    "print(f\"- Spam probability vector: {len(p1V)} features\")\n",
    "print(f\"- Prior spam probability: {pSpam:.3f}\")\n",
    "print(f\"- Vocabulary size: {len(vocab_array)}\")\n",
    "print(f\"- Model file: models/spam_classifier_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8659ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model loading...\n",
      "Model loaded successfully!\n",
      "Loaded components:\n",
      "- p0_vector: shape (7270,)\n",
      "- p1_vector: shape (7270,)\n",
      "- p_spam: 0.1350684316805026\n",
      "- vocabulary: shape (7270,)\n",
      "- word_to_idx: 7270 items\n",
      "- stop_words: 198 items\n",
      "\n",
      "Testing loaded model with: 'FREE! Win $1000 now! Limited time offer!'\n",
      "Prediction: SPAM\n",
      "Model loading and testing successful!\n"
     ]
    }
   ],
   "source": [
    "# Test loading the saved model\n",
    "print(\"Testing model loading...\")\n",
    "\n",
    "with open('models/spam_classifier_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(\"Loaded components:\")\n",
    "for key, value in loaded_model.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"- {key}: shape {value.shape}\")\n",
    "    elif isinstance(value, (dict, set)):\n",
    "        print(f\"- {key}: {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "# Test the loaded model with a sample message\n",
    "test_msg = \"FREE! Win $1000 now! Limited time offer!\"\n",
    "print(f\"\\nTesting loaded model with: '{test_msg}'\")\n",
    "\n",
    "# Use loaded model components\n",
    "loaded_p0V = loaded_model['p0_vector']\n",
    "loaded_p1V = loaded_model['p1_vector']\n",
    "loaded_pSpam = loaded_model['p_spam']\n",
    "loaded_vocab = loaded_model['vocabulary']\n",
    "loaded_word_to_idx = loaded_model['word_to_idx']\n",
    "loaded_stop_words = loaded_model['stop_words']\n",
    "\n",
    "# Clean and vectorize test message\n",
    "def clean_text_loaded(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in loaded_stop_words and len(w) > 3]\n",
    "    return words\n",
    "\n",
    "def vectorize_message_loaded(message):\n",
    "    vec = np.zeros(len(loaded_vocab), dtype=int)\n",
    "    for w in message:\n",
    "        if w in loaded_word_to_idx:\n",
    "            vec[loaded_word_to_idx[w]] += 1\n",
    "    return vec\n",
    "\n",
    "cleaned = clean_text_loaded(test_msg)\n",
    "vectorized = vectorize_message_loaded(cleaned)\n",
    "prediction = classify_NB(vectorized, loaded_p0V, loaded_p1V, loaded_pSpam)\n",
    "\n",
    "print(f\"Prediction: {'SPAM' if prediction == 1 else 'HAM'}\")\n",
    "print(\"Model loading and testing successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6db933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
